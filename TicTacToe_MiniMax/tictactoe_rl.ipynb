{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57de1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debdi\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15800e8",
   "metadata": {},
   "source": [
    "## We will be taking X's as 1 and O's as -1 and by default empty is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c4614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board[:] = 0\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        i, j = action\n",
    "        if self.board[i, j] != 0:\n",
    "            raise ValueError(\"Invalid action: Cell is not empty\")\n",
    "            \n",
    "        self.board[i, j] = self.current_player\n",
    "        done, winner = self.is_done()\n",
    "        reward = self.get_reward(done, winner)\n",
    "        \n",
    "        self.current_player *= -1\n",
    "        return self.get_state(), reward, done, {\"Winner\": winner}\n",
    "    \n",
    "    def is_done(self):\n",
    "        for i in range(3):\n",
    "            if abs(sum(self.board[i, :])) == 3:\n",
    "                return True, np.sign(sum(self.board[i, :]))\n",
    "            if abs(sum(self.board[:, i])) == 3:\n",
    "                return True, np.sign(sum(self.board[:, i]))\n",
    "        if abs(sum([self.board[i, i] for i in range(3)])) == 3:\n",
    "            return True, np.sign(sum([self.board[i, i] for i in range(3)]))\n",
    "        if abs(sum([self.board[i, 2 - i] for i in range(3)])) == 3:\n",
    "            return True, np.sign(sum([self.board[i, 2 - i] for i in range(3)]))\n",
    "        \n",
    "        if not self.get_valid_actions():\n",
    "            return True, 0 # DRAW\n",
    "        \n",
    "        return False, None\n",
    "    \n",
    "    def get_reward(self, done, winner):\n",
    "        if not done:\n",
    "            return 0\n",
    "        if winner == 1:\n",
    "            return 1\n",
    "        elif winner == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0.5 # draw\n",
    "        \n",
    "    def render(self):\n",
    "        symbols = {1: 'X', -1: 'O', 0: '.'}\n",
    "        for row in self.board:\n",
    "            print(\" \".join(symbols[val] for val in row))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3846c5",
   "metadata": {},
   "source": [
    "## Testing the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca0cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv()\n",
    "\n",
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3002ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 plays (0, 1)\n",
      ". X .\n",
      ". . .\n",
      ". . .\n",
      "\n",
      "Player -1 plays (2, 1)\n",
      ". X .\n",
      ". . .\n",
      ". O .\n",
      "\n",
      "Player 1 plays (0, 0)\n",
      "X X .\n",
      ". . .\n",
      ". O .\n",
      "\n",
      "Player -1 plays (1, 1)\n",
      "X X .\n",
      ". O .\n",
      ". O .\n",
      "\n",
      "Player 1 plays (2, 2)\n",
      "X X .\n",
      ". O .\n",
      ". O X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    valid_actions = env.get_valid_actions()\n",
    "    action = random.choice(valid_actions)\n",
    "    print(f\"Player {env.current_player} plays {action}\")\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        if info[\"winner\"] == 1:\n",
    "            print(\"X wins!\")\n",
    "        elif info[\"winner\"] == -1:\n",
    "            print(\"O wins!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19298d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c7b55",
   "metadata": {},
   "source": [
    "## Creating Gym Compatible Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165f257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeOpponentEnv(gym.Env):\n",
    "    def __init__(self, opponent):\n",
    "        super(TicTacToeOpponentEnv, self).__init__()\n",
    "        self.env = TicTacToeEnv()\n",
    "        self.opponent = opponent\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(3, 3), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # sets the RNG if needed\n",
    "\n",
    "        state = self.env.reset()\n",
    "\n",
    "        if self.env.current_player == -1:\n",
    "            opp_action = self.opponent.select_action(state, self.env.get_valid_actions())\n",
    "            self.env.step(opp_action)\n",
    "            state = self.env.get_state()\n",
    "\n",
    "        return state, {} \n",
    "    \n",
    "    def step(self, action_index):\n",
    "        valid_actions = self.env.get_valid_actions()\n",
    "\n",
    "        row, col = divmod(action_index, 3)\n",
    "\n",
    "        if (row, col) not in valid_actions:\n",
    "            reward = -2.5\n",
    "            row, col = valid_actions[np.random.randint(len(valid_actions))]\n",
    "        else:\n",
    "            reward = 0 \n",
    "        \n",
    "        state, step_reward, done, info = self.env.step((row, col))\n",
    "        \n",
    "        reward += step_reward\n",
    "\n",
    "        if not done and self.env.current_player == -1:\n",
    "            opp_action = self.opponent.select_action(state, self.env.get_valid_actions())\n",
    "            state, reward, done, info = self.env.step(opp_action)\n",
    "\n",
    "            if done:\n",
    "                reward *= -1\n",
    "\n",
    "        return state, reward, done, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf4cfd",
   "metadata": {},
   "source": [
    "## Logs saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5d0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./ppo_logs\"\n",
    "model_dir = \"./saved_models\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a1930",
   "metadata": {},
   "source": [
    "## Testing against Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db79b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_model(model, env):\n",
    "    inner_env = env.envs[0].env  # unwrap the actual TicTacToeOpponentEnv\n",
    "    obs = env.reset()[0]         # unwrap observation\n",
    "    done = False\n",
    "\n",
    "    print(\"You are 'O' (represented by -1), model is 'X' (represented by 1)\")\n",
    "    print(\"Cell numbers:\\n1 | 2 | 3\\n4 | 5 | 6\\n7 | 8 | 9\\n\")\n",
    "\n",
    "    while not done:\n",
    "        if inner_env.current_player == -1:\n",
    "            # Human's turn\n",
    "            valid = inner_env.get_valid_actions()\n",
    "            inner_env.render()\n",
    "            print(f\"Valid moves: {[r * 3 + c + 1 for r, c in valid]}\")\n",
    "            while True:\n",
    "                try:\n",
    "                    move = int(input(\"Enter your move (1-9): \")) - 1\n",
    "                    action = (move // 3, move % 3)\n",
    "                    if action in valid:\n",
    "                        break\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a number between 1 and 9.\")\n",
    "            obs, reward, done, _ = inner_env.step(action)\n",
    "        else:\n",
    "            # Model's turn\n",
    "            action_flat, _ = model.predict(obs)\n",
    "            action_flat = int(action_flat)\n",
    "            action = (action_flat // 3, action_flat % 3)\n",
    "            obs, reward, done, _ = inner_env.step(action)\n",
    "\n",
    "    inner_env.render()\n",
    "    if reward == 1:\n",
    "        print(\"Model won!\")\n",
    "    elif reward == -1:\n",
    "        print(\"You won!\")\n",
    "    else:\n",
    "        print(\"It's a draw!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58f58",
   "metadata": {},
   "source": [
    "## 1. Random Agent vs RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c15e9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxAgent:\n",
    "    def __init__(self):\n",
    "        self.memo = {}\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        best_score = float('-inf')\n",
    "        best_action = None\n",
    "        \n",
    "        for action in valid_actions:\n",
    "            next_state = self.apply_action(state, action, player=1)  # MinMaxAgent is 1\n",
    "            score = self.minimax(next_state, maximizing=False)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "\n",
    "    def minimax(self, state, maximizing):\n",
    "        # Check if the state has already been evaluated\n",
    "        state_tuple = tuple(map(tuple, state))  # Convert to tuple for immutability and hashing\n",
    "        if state_tuple in self.memo:\n",
    "            return self.memo[state_tuple]  # Return cached result\n",
    "\n",
    "        # Evaluate the current state\n",
    "        winner = self.check_winner(state)\n",
    "        if winner == 1:\n",
    "            self.memo[state_tuple] = 1.0\n",
    "            return 1.0\n",
    "        elif winner == -1:\n",
    "            self.memo[state_tuple] = -1.0\n",
    "            return -1.0\n",
    "        elif self.is_full(state):\n",
    "            self.memo[state_tuple] = 0.5\n",
    "            return 0.5\n",
    "\n",
    "        valid_actions = self.get_valid_actions(state)\n",
    "\n",
    "        if maximizing:\n",
    "            max_eval = float('-inf')\n",
    "            for action in valid_actions:\n",
    "                next_state = self.apply_action(state, action, player=1)\n",
    "                eval = self.minimax(next_state, maximizing=False)\n",
    "                max_eval = max(max_eval, eval)\n",
    "            self.memo[state_tuple] = max_eval\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for action in valid_actions:\n",
    "                next_state = self.apply_action(state, action, player=-1)\n",
    "                eval = self.minimax(next_state, maximizing=True)\n",
    "                min_eval = min(min_eval, eval)\n",
    "            self.memo[state_tuple] = min_eval\n",
    "            return min_eval\n",
    "\n",
    "    def apply_action(self, state, action, player):\n",
    "        next_state = copy.deepcopy(state)\n",
    "        i, j = action\n",
    "        next_state[i][j] = player\n",
    "        return next_state\n",
    "\n",
    "    def get_valid_actions(self, state):\n",
    "        actions = []\n",
    "        for i in range(len(state)):\n",
    "            for j in range(len(state[0])):\n",
    "                if state[i][j] == 0:  # empty cells are 0, not '-'\n",
    "                    actions.append((i, j))\n",
    "        return actions\n",
    "\n",
    "    def is_full(self, state):\n",
    "        for row in state:\n",
    "            for cell in row:\n",
    "                if cell == 0:  # empty cells are 0\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def check_winner(self, state):\n",
    "        N = len(state)\n",
    "        for i in range(N):\n",
    "            if state[i, 0] != 0 and all(state[i, j] == state[i, 0] for j in range(N)):\n",
    "                return state[i, 0]\n",
    "            if state[0, i] != 0 and all(state[j, i] == state[0, i] for j in range(N)):\n",
    "                return state[0, i]\n",
    "        if state[0, 0] != 0 and all(state[i, i] == state[0, 0] for i in range(N)):\n",
    "            return state[0, 0]\n",
    "        if state[0, N-1] != 0 and all(state[i, N-1-i] == state[0, N-1] for i in range(N)):\n",
    "            return state[0, N-1]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4290916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_logs\\PPO_5\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.73     |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.79        |\n",
      "|    ep_rew_mean          | -0.005      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 960         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013399223 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | -0.0138     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.603       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.67       |\n",
      "|    ep_rew_mean          | 0.185      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 897        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01553121 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.15      |\n",
      "|    explained_variance   | 0.147      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.457      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    value_loss           | 1.26       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.39        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013616128 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.1        |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.473       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.38        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014659354 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.924       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.28        |\n",
      "|    ep_rew_mean          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014063974 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.712       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.37        |\n",
      "|    ep_rew_mean          | 0.895       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013812451 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.562       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.23        |\n",
      "|    ep_rew_mean          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014812878 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.315       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.22        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018298373 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.26        |\n",
      "|    ep_rew_mean          | 0.925       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014091177 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.146      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0028      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.2         |\n",
      "|    ep_rew_mean          | 0.975       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015729595 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00306    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.23        |\n",
      "|    ep_rew_mean          | 0.995       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010877954 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.21       |\n",
      "|    ep_rew_mean          | 0.975      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 861        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01464663 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.0977     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0371     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    value_loss           | 0.0737     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.22        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011267003 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -0.0792     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.0285      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.18         |\n",
      "|    ep_rew_mean          | 0.92         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075865234 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.0658       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00606     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.0503       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.22        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014039572 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.0424      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.23       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 852        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 40         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01869352 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.254      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00749   |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 0.00671    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.14        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012101334 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -32.6       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.09        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017914023 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.123      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00447    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.009       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.06        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031298995 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | -0.00848    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0624     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.00832     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014105683 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0489     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.0211      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.03        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020285124 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.832      |\n",
      "|    explained_variance   | -11.3       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.000601    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 0.975       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005643835 |\n",
      "|    clip_fraction        | 0.0663      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.782      |\n",
      "|    explained_variance   | -0.318      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 5.9e-05     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.01         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040985136 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.774       |\n",
      "|    explained_variance   | 0.062        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    value_loss           | 0.0251       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.02       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 864        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01615756 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.732     |\n",
      "|    explained_variance   | -0.154     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0192     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00846   |\n",
      "|    value_loss           | 0.00849    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3          |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 864        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 61         |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01793614 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.719     |\n",
      "|    explained_variance   | -1.01      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00699   |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00791   |\n",
      "|    value_loss           | 7.93e-05   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.02         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077867922 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.737       |\n",
      "|    explained_variance   | -14.6        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00518      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00946     |\n",
      "|    value_loss           | 0.000511     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012427764 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00499    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 0.00684     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.01       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 855        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258823 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.722     |\n",
      "|    explained_variance   | -13.2      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    value_loss           | 0.000266   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.02        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 853         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017828558 |\n",
      "|    clip_fraction        | 0.0822      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.752      |\n",
      "|    explained_variance   | -0.01       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.00857     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018637965 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.724      |\n",
      "|    explained_variance   | -0.0673     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0658     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 0.00837     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029697701 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.801      |\n",
      "|    explained_variance   | 0.00429     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0245      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.0168      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.02        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029438576 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | -15.5       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.000774    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014546368 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | -1.92       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00787    |\n",
      "|    value_loss           | 0.000151    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011006016 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 1.45e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 864         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014777942 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.02        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029367847 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | -0.0668     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00266     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 4.7e-05     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3          |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 867        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 89         |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02993447 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.681     |\n",
      "|    explained_variance   | 0.00496    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00308   |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00868   |\n",
      "|    value_loss           | 0.00821    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.01       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 868        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02623739 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.613     |\n",
      "|    explained_variance   | -0.542     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0506    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.00837   |\n",
      "|    value_loss           | 3.97e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013690883 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | -0.0331     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 5.16e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 3         |\n",
      "|    ep_rew_mean          | 1         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 865       |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 96        |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0059626 |\n",
      "|    clip_fraction        | 0.068     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.579    |\n",
      "|    explained_variance   | 0.908     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00881   |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.00255  |\n",
      "|    value_loss           | 1.92e-06  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037952196 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0011     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 4.91e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017848782 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 7.14e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008371286 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | -0.0135     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010702359 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | -0.885      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00697     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 4.39e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009603852 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.0436      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0286     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    value_loss           | 0.00942     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 864         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005546028 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | -6.6        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00338    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00527    |\n",
      "|    value_loss           | 8.45e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017458394 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00574    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00973    |\n",
      "|    value_loss           | 0.00848     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.01       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 865        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03119769 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | -6.16      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00695   |\n",
      "|    value_loss           | 0.000108   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x18ace8b1f50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_opponent = MinMaxAgent()\n",
    "\n",
    "monitored_env = Monitor(TicTacToeOpponentEnv(random_opponent), log_dir)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=25000, save_path=model_dir, name_prefix=\"ppo_model\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", monitored_env, verbose=1, tensorboard_log=log_dir)\n",
    "\n",
    "model.learn(total_timesteps=100_000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are 'O' (represented by -1), model is 'X' (represented by 1)\n",
      "Cell numbers:\n",
      "1 | 2 | 3\n",
      "4 | 5 | 6\n",
      "7 | 8 | 9\n",
      "\n",
      ". . .\n",
      ". X .\n",
      ". . .\n",
      "\n",
      "Valid moves: [1, 2, 3, 4, 6, 7, 8, 9]\n",
      "Enter your move (1-9): 3\n",
      ". . O\n",
      ". X .\n",
      ". X .\n",
      "\n",
      "Valid moves: [1, 2, 4, 6, 7, 9]\n",
      "Enter your move (1-9): 2\n",
      ". O O\n",
      ". X .\n",
      ". X X\n",
      "\n",
      "Valid moves: [1, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"./saved_models/ppo_model_100000_steps.zip\")\n",
    "\n",
    "opponent = MinMaxAgent()\n",
    "opponent_env = TicTacToeOpponentEnv(opponent)\n",
    "vec_env = DummyVecEnv([lambda: opponent_env])\n",
    "\n",
    "play_against_model(model, vec_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
