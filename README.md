# ğŸ§  Tic Tac Toe with Reinforcement Learning and MiniMax

This project implements a Tic Tac Toe game using two intelligent agents:

* A **Reinforcement Learning (RL)** agent trained via Q-Learning.
* An agent based on the **MiniMax algorithm** with memoization for optimization.

It showcases how AI can learn and optimize decision-making in classic games using two powerful paradigms: model-free RL and adversarial search.

---

## ğŸ“‚ Project Structure

| File                      | Description                                                                                                |
| ------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `tictactoe_rl.ipynb`      | Implements Q-learning for training and simulating RL agents playing against each other.                    |
| `tictactoe_minimax.ipynb` | Implements the MiniMax algorithm with memoization to play optimally against the RL agent or human players. |

---

## ğŸ® Features

* âœ… RL vs RL gameplay: two agents learn optimal strategies via self-play.
* âœ… RL agent trained using Q-learning with Îµ-greedy policy.
* âœ… MiniMax agent with pruning and memoization for speed.
* âœ… Visual and tabular result tracking (wins, losses, draws).
* âœ… Interactive step-by-step gameplay in Jupyter Notebook.

---

## ğŸ“Š Technologies Used

* Python
* Jupyter Notebook
* NumPy
* Matplotlib (for visualization, if used)

---

## ğŸ§  Reinforcement Learning Approach

* **Q-learning** with state-action mapping (`Q-table`).
* Exploration-exploitation tradeoff using Îµ-greedy strategy.
* Reward structure:

  * Win: +1
  * Draw: 0
  * Loss: -1
* Agent learns through **self-play** and iterative Q-table updates.

---

## â™Ÿï¸ MiniMax Algorithm

* Deterministic and adversarial decision-making.
* Explores all possible moves and counter-moves.
* Memoization to avoid redundant recalculations.
* Always plays the optimal move.

---

## âœ… Results Summary

| Matchup            | Outcome                                                      |
| ------------------ | ------------------------------------------------------------ |
| RL vs RL           | Agents converge to near-optimal strategy over episodes.      |
| RL vs MiniMax      | MiniMax usually wins unless RL has enough training episodes. |
| MiniMax vs MiniMax | Always ends in a draw due to perfect play.                   |

---

## ğŸ“Œ Future Improvements

* Add GUI using `pygame` or `tkinter`.
* Incorporate Deep Q-Network (DQN) for larger boards.
* Add human vs AI interactive mode.

